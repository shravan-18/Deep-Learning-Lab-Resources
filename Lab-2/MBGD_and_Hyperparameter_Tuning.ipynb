{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtp5Hq9aY3Kc"
      },
      "source": [
        "# **Install Dependencies and Ignore Warnings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ugLo5zdN9Uz"
      },
      "outputs": [],
      "source": [
        "# No need to run this cell on colab since it already has these installed\n",
        "!pip install tensorflow numpy scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSknvO9oXmnx"
      },
      "outputs": [],
      "source": [
        "# Install scikeras for compatibility between Scikit-learn and Keras.\n",
        "!pip install -q scikeras -q scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eC8foH_-b5Py"
      },
      "outputs": [],
      "source": [
        "# Import warnings module to filter out any warning messages for a cleaner output.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8R4Ta9oanT7"
      },
      "source": [
        "# **Create Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdmSwWNmMVU-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Generate synthetic classification data with 1000 samples and 20 features.\n",
        "# This dataset will be used for training and evaluation.\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "print(\"Inputs shape: \", X.shape)\n",
        "print(\"Outputs shape: \", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxrpsFLscVCz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Perform Principal Component Analysis (PCA) to reduce data dimensions to 2 for visualization.\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Convert the PCA transformed data into a DataFrame for easier plotting.\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
        "df['Label'] = y\n",
        "\n",
        "# Plot the 2D visualization of the dataset, differentiating the classes using colors.\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='PC1', y='PC2', hue='Label', palette='viridis', data=df)\n",
        "plt.title('2D Visualization of the Synthetic Dataset')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPk87AT8ap4q"
      },
      "source": [
        "# **Build Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_J-dwdXtMZHe"
      },
      "outputs": [],
      "source": [
        "'''Define the ANN Model'''\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "\n",
        "# Define the ANN Model\n",
        "def create_model(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)):\n",
        "    \"\"\"\n",
        "    Creates and compiles a simple neural network model.\n",
        "\n",
        "    Args:\n",
        "    optimizer: The optimizer to use for training the model. Default is Adam with a learning rate of 0.01.\n",
        "\n",
        "    Returns:\n",
        "    A compiled Keras model ready for training.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, activation='relu', input_dim=20))  # Input layer with 20 features\n",
        "    model.add(Dense(16, activation='relu'))                # Hidden layer with 16 units\n",
        "    model.add(Dense(1, activation='sigmoid'))              # Output layer for binary classification\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYz9nf09asNG"
      },
      "source": [
        "# **Grid Search for Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxa7xl6JMZKj"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Wrap the Keras model in Scikit-learn's KerasClassifier for compatibility with GridSearchCV.\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# Define the grid search parameters, including different optimizers, batch sizes, and epochs.\n",
        "param_grid = {'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "              'batch_size': [16, 32, 64],\n",
        "              'epochs': [10, 20]}\n",
        "\n",
        "# Initialize GridSearchCV to perform an exhaustive search over specified parameter values.\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2)\n",
        "\n",
        "# Fit the grid search model to find the best hyperparameters.\n",
        "print(\"Performing Grid Search for Hyperparameter Tuning...\\n\")\n",
        "grid_result = grid.fit(X, y) # This step might take some time to execute.\n",
        "\n",
        "# Print the best accuracy and parameters obtained from the grid search.\n",
        "print(f\"\\nBest Accuracy achieved: {grid_result.best_score_} using {grid_result.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYiRv1znczZW"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "# Convert the GridSearchCV results to a DataFrame for visualization.\n",
        "cv_results = pd.DataFrame(grid_result.cv_results_)\n",
        "\n",
        "# Flatten the 'params' column for easier visualization.\n",
        "params = cv_results['params'].apply(pd.Series)\n",
        "\n",
        "# Concatenate the flattened params with the original DataFrame.\n",
        "results_df = pd.concat([cv_results.drop(['params'], axis=1), params], axis=1)\n",
        "\n",
        "# Define hyperparameters for which to generate pairwise combinations.\n",
        "hyperparams = ['optimizer', 'epochs', 'batch_size']\n",
        "\n",
        "# Create all unique pairs of hyperparameters for visualization.\n",
        "hyperparam_combinations = list(combinations(hyperparams, 2))\n",
        "\n",
        "# Plot heatmaps for each pairwise combination of hyperparameters.\n",
        "fig, axs = plt.subplots(1, 3, figsize=(24, 8))  # Create a figure with 3 subplots side by side\n",
        "\n",
        "for idx, combo in enumerate(hyperparam_combinations):\n",
        "    param1, param2 = combo\n",
        "\n",
        "    # Pivot the DataFrame to prepare for the heatmap.\n",
        "    pivot_df = results_df.pivot_table(index=param1, columns=param2, values='mean_test_score', aggfunc=np.mean)\n",
        "\n",
        "    # Fill missing values, if any, with 0.\n",
        "    pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Plot the heatmap on the corresponding subplot.\n",
        "    sns.heatmap(pivot_df, ax=axs[idx], annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "    axs[idx].set_title(f'Grid Search CV Results: {param1} vs {param2}')\n",
        "    axs[idx].set_xlabel(param2)  # Label for the x-axis\n",
        "    axs[idx].set_ylabel(param1)  # Label for the y-axis\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jCE40GHbDfS"
      },
      "source": [
        "# **Random Search for Hyperparameter Tuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpH8THnqMZMg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Reinitialize the Keras model for RandomizedSearchCV.\n",
        "model = KerasClassifier(build_fn=create_model(tf.keras.optimizers.Adam(learning_rate=0.01)), verbose=0)\n",
        "\n",
        "# Define the random search parameters.\n",
        "param_dist = {'optimizer': ['adam', 'rmsprop', 'sgd'],\n",
        "              'batch_size': [16, 32, 64],\n",
        "              'epochs': [10, 20]}\n",
        "\n",
        "# Initialize RandomizedSearchCV to randomly sample the parameter space.\n",
        "rand = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_jobs=-1)\n",
        "\n",
        "# Fit the random search model to find the best hyperparameters.\n",
        "print(\"Performing Randomized Search for Hyperparameter Tuning...\\n\")\n",
        "rand_result = rand.fit(X, y)\n",
        "\n",
        "# Print the best accuracy and parameters obtained from the random search.\n",
        "print(f\"Best: {rand_result.best_score_} using {rand_result.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVxBD5IWem4C"
      },
      "outputs": [],
      "source": [
        "# Convert the RandomizedSearchCV results to a DataFrame for visualization.\n",
        "rand_results = pd.DataFrame(rand_result.cv_results_)\n",
        "\n",
        "# Flatten the 'params' column for easier visualization.\n",
        "params = rand_results['params'].apply(pd.Series)\n",
        "\n",
        "# Concatenate the flattened params with the original DataFrame.\n",
        "results_df = pd.concat([rand_results.drop(['params'], axis=1), params], axis=1)\n",
        "\n",
        "# Define hyperparameters for which to generate pairwise combinations.\n",
        "hyperparams = ['optimizer', 'epochs', 'batch_size']\n",
        "\n",
        "# Create all unique pairs of hyperparameters for visualization.\n",
        "hyperparam_combinations = list(combinations(hyperparams, 2))\n",
        "\n",
        "# Plot heatmaps for each pairwise combination of hyperparameters.\n",
        "fig, axs = plt.subplots(1, 3, figsize=(24, 8))  # Create a figure with 3 subplots side by side\n",
        "\n",
        "for idx, combo in enumerate(hyperparam_combinations):\n",
        "    param1, param2 = combo\n",
        "\n",
        "    # Pivot the DataFrame to prepare for the heatmap.\n",
        "    pivot_df = results_df.pivot_table(index=param1, columns=param2, values='mean_test_score', aggfunc=np.mean)\n",
        "\n",
        "    # Fill missing values, if any, with 0.\n",
        "    pivot_df.fillna(0, inplace=True)\n",
        "\n",
        "    # Plot the heatmap on the corresponding subplot.\n",
        "    sns.heatmap(pivot_df, ax=axs[idx], annot=True, fmt=\".2f\", cmap=\"YlGnBu\")\n",
        "    axs[idx].set_title(f'Grid Search CV Results: {param1} vs {param2}')\n",
        "    axs[idx].set_xlabel(param2)  # Label for the x-axis\n",
        "    axs[idx].set_ylabel(param1)  # Label for the y-axis\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B4fBZKYbdh0"
      },
      "source": [
        "# **Learning Rate (LR) Scheduler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzBP2PDnbQia"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Create a new model instance with a higher initial learning rate.\n",
        "model = create_model(optimizer=Adam(learning_rate=0.1))\n",
        "\n",
        "# Define a callback to reduce the learning rate when the validation loss plateaus.\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
        "\n",
        "# Fit the model with the learning rate scheduler callback.\n",
        "history = model.fit(X, y, validation_split=0.2, epochs=64, batch_size=32, callbacks=[reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_3PpKB9fAyn"
      },
      "outputs": [],
      "source": [
        "# Extract and plot the learning rate history over epochs.\n",
        "lr_history = history.history['learning_rate']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(lr_history)\n",
        "plt.title('Learning Rate Scheduler Performance')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncIOXJtnboYH"
      },
      "source": [
        "# **Early Stopping**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UueRAVINboel"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Create a new model instance for training with early stopping.\n",
        "model = create_model(optimizer=Adam(learning_rate=0.01))\n",
        "\n",
        "# Define an early stopping callback to prevent overfitting.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Fit the model with the early stopping callback.\n",
        "history = model.fit(X, y, validation_split=0.2, epochs=64, batch_size=32, callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbLarQ5d6yXb"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation loss over epochs to visualize the impact of early stopping.\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss with Early Stopping')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgGD3nug_1nc"
      },
      "source": [
        "# **Inference and Conclusion**\n",
        "\n",
        "### Minibatch Gradient Descent (MBGD)\n",
        "\n",
        "#### Batch Sizes:\n",
        "- **Impact of Batch Size**: Different batch sizes can have significant impacts on model performance.\n",
        "  - **Smaller Batch Sizes**: Often provide a more accurate estimate of the gradient, leading to more frequent updates and potentially faster convergence. However, they can introduce more noise into the training process.\n",
        "  - **Larger Batch Sizes**: Provide a smoother estimate of the gradient, which can lead to more stable convergence. They typically require more memory and can slow down the convergence process.\n",
        "\n",
        "### Hyperparameter Tuning\n",
        "\n",
        "#### Grid Search:\n",
        "- **Best Parameters**: The Grid Search identified the best combination of hyperparameters to achieve optimal model performance.\n",
        "- **Heatmaps**: The heatmaps provided a visual understanding of how different hyperparameter combinations affect model performance.\n",
        "- **Accuracy**: The best accuracy achieved was noted along with the specific hyperparameters used.\n",
        "\n",
        "#### Random Search:\n",
        "- **Efficiency**: Random Search was faster compared to Grid Search as it evaluates a random subset of hyperparameter combinations rather than all possible combinations.\n",
        "- **Best Parameters**: The Random Search also identified a set of hyperparameters that yielded a high accuracy.\n",
        "- **Comparison**: Both Grid and Random Search are effective, but Random Search can be more efficient in terms of computation time, especially with larger hyperparameter spaces.\n",
        "\n",
        "### Learning Rate Scheduler\n",
        "- **Performance**: The learning rate scheduler dynamically adjusted the learning rate during training, which helped in achieving better convergence.\n",
        "- **Plot**: The learning rate plot illustrated how the learning rate was reduced over epochs, which contributed to stabilizing the training process.\n",
        "\n",
        "### Early Stopping\n",
        "- **Prevention of Overfitting**: Early stopping halted training when the validation loss stopped improving, preventing the model from overfitting.\n",
        "- **Efficiency**: This method saved computational resources by stopping training early when further training would not improve the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
